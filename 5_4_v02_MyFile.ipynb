{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = np.loadtxt('train.csv', delimiter=',', skiprows=1)\n",
    "test = np.loadtxt('test.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "#train = np.loadtxt('/media/ivan/778B482205074FE31/data/mnist/train.csv', delimiter=',', skiprows=1)\n",
    "#test = np.loadtxt('/media/ivan/778B482205074FE31/data/mnist/test.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сохраняем разметку в отдельную переменную\n",
    "train_label = train[:, 0]\n",
    "# приводим размерность к удобному для обаботки виду\n",
    "train_img = np.resize(train[:, 1:], (train.shape[0], 28, 28))\n",
    "test_img = np.resize(test, (test.shape[0], 28, 28))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Порядок решения\n",
    "\n",
    "Если вычислять с делением от самого начала\n",
    "1. Делим области на 4 части\n",
    "    2. Вычисляем составляющие градиента для каждой области\n",
    "    3. Угол и длина вектора градиента для каждой области\n",
    "    4. Вычисляем гистограмму градиента для каждой области\n",
    "5. Стыковка данных всех областей в один ммассив\n",
    "6. Нормирование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "#1\n",
    "chunks =  [train_img[:, :14, :14], train_img[:, 14:, :14], train_img[:, :14, 14:], train_img[:, 14:, 14:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2\n",
    "chunks_final = np.zeros((len(train_img), 1))\n",
    "\n",
    "for i in range(len(chunks)):\n",
    "    chunks_sobel_x = np.zeros_like(chunks[i] )\n",
    "    chunks_sobel_y = np.zeros_like(chunks[i] )\n",
    "    for m in range(len(chunks[i])):\n",
    "        chunks_sobel_x[m] = cv2.Sobel(chunks[i][m], cv2.CV_64F, dx=1, dy=0, ksize=3)\n",
    "        chunks_sobel_y[m] = cv2.Sobel(chunks[i][m], cv2.CV_64F, dx=0, dy=1, ksize=3)\n",
    "    chunks_g, chunks_theta = cv2.cartToPolar(chunks_sobel_x, chunks_sobel_y)      #3\n",
    "    \n",
    "    chunks_hist = np.zeros((len(chunks[i]), 16))   #4\n",
    "    for x in range(len(chunks[i])):\n",
    "        hist, borders = np.histogram(chunks_theta[x],\n",
    "                                     bins=16,\n",
    "                                     range=(0., 2. * np.pi),\n",
    "                                     weights=chunks_g[x])\n",
    "        chunks_hist[x] = hist\n",
    "    chunks_final = np.hstack((chunks_final, chunks_hist))\n",
    "\n",
    "chunks_final = chunks_final[:, 1:] #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "#1\n",
    "chunks_tst =  [test_img[:, :14, :14], test_img[:, 14:, :14], test_img[:, :14, 14:], test_img[:, 14:, 14:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2\n",
    "chunks_final_tst = np.zeros((len(test_img), 1))\n",
    "\n",
    "for i in range(len(chunks_tst)):\n",
    "    chunks_sobel_x = np.zeros_like(chunks_tst[i] )\n",
    "    chunks_sobel_y = np.zeros_like(chunks_tst[i] )\n",
    "    for m in range(len(chunks_tst[i])):\n",
    "        chunks_sobel_x[m] = cv2.Sobel(chunks_tst[i][m], cv2.CV_64F, dx=1, dy=0, ksize=3)\n",
    "        chunks_sobel_y[m] = cv2.Sobel(chunks_tst[i][m], cv2.CV_64F, dx=0, dy=1, ksize=3)\n",
    "    chunks_g, chunks_theta = cv2.cartToPolar(chunks_sobel_x, chunks_sobel_y)      #3\n",
    "    \n",
    "    chunks_hist = np.zeros((len(chunks_tst[i]), 16))\n",
    "    for x in range(len(chunks_tst[i])):\n",
    "        hist, borders = np.histogram(chunks_theta[x],\n",
    "                                     bins=16,\n",
    "                                     range=(0., 2. * np.pi),\n",
    "                                     weights=chunks_g[x])\n",
    "        chunks_hist[x] = hist\n",
    "    chunks_final_tst = np.hstack((chunks_final_tst, chunks_hist))\n",
    "\n",
    "chunks_final_tst = chunks_final_tst[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# нормализация методом из opencv\n",
    "train_zero = np.zeros((42000, 64))\n",
    "train_norm = cv2.normalize(chunks_final, train_zero)\n",
    "test_norm = cv2.normalize(chunks_final_tst, train_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиваем выборку на обучение и валидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y_train, y_val, x_train, x_val = train_test_split(\n",
    "    train_label, train_norm, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собираем полносвязную сеть для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(256, input_dim=x_train.shape[1], activation='relu'\n",
    "#                            ,kernel_initializer='random_uniform'\n",
    "                            )) # \n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(keras.layers.Dense(256, input_dim=x_train.shape[1], activation='relu'\n",
    "#                            ,kernel_initializer='random_uniform'\n",
    "                            )) # \n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "#model.add(keras.layers.Dense(256, input_dim=x_train.shape[1], activation='relu')) # \n",
    "#model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax')) # softmax сколько классов == столько выходных нейронов\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводим информацию о модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 85,002\n",
      "Trainable params: 85,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train_labels = np_utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33600,) (33600, 10) (42000, 64)\n"
     ]
    }
   ],
   "source": [
    "print( y_train.shape, y_train_labels.shape, train_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "33600/33600 [==============================] - 1s - loss: 2.2542 - acc: 0.1607     \n",
      "Epoch 2/250\n",
      "33600/33600 [==============================] - 1s - loss: 1.7296 - acc: 0.3805     \n",
      "Epoch 3/250\n",
      "33600/33600 [==============================] - 1s - loss: 1.2719 - acc: 0.5716     \n",
      "Epoch 4/250\n",
      "33600/33600 [==============================] - 1s - loss: 1.0071 - acc: 0.6650     \n",
      "Epoch 5/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.8320 - acc: 0.7238     \n",
      "Epoch 6/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.7139 - acc: 0.7633     \n",
      "Epoch 7/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.6360 - acc: 0.7932     \n",
      "Epoch 8/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.5750 - acc: 0.8118     \n",
      "Epoch 9/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.5300 - acc: 0.8287     \n",
      "Epoch 10/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.4914 - acc: 0.8399     \n",
      "Epoch 11/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.4632 - acc: 0.8511     - ETA: 0s - loss: 0.4683 - acc:\n",
      "Epoch 12/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.4422 - acc: 0.8571     \n",
      "Epoch 13/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.4209 - acc: 0.8656     \n",
      "Epoch 14/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.3920 - acc: 0.8763     \n",
      "Epoch 15/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.3818 - acc: 0.8801     \n",
      "Epoch 16/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.3670 - acc: 0.8849     \n",
      "Epoch 17/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.3512 - acc: 0.8918     \n",
      "Epoch 18/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.3360 - acc: 0.8947     \n",
      "Epoch 19/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.3251 - acc: 0.9001     \n",
      "Epoch 20/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.3131 - acc: 0.9033     \n",
      "Epoch 21/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.3073 - acc: 0.9054     \n",
      "Epoch 22/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2981 - acc: 0.9076     \n",
      "Epoch 23/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2870 - acc: 0.9118     \n",
      "Epoch 24/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2823 - acc: 0.9131     \n",
      "Epoch 25/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2756 - acc: 0.9144     \n",
      "Epoch 26/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2748 - acc: 0.9157     \n",
      "Epoch 27/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2656 - acc: 0.9184     \n",
      "Epoch 28/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2644 - acc: 0.9185     \n",
      "Epoch 29/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2562 - acc: 0.9207     \n",
      "Epoch 30/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2508 - acc: 0.9209     \n",
      "Epoch 31/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2474 - acc: 0.9235     \n",
      "Epoch 32/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2484 - acc: 0.9212     \n",
      "Epoch 33/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2402 - acc: 0.9256     \n",
      "Epoch 34/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2382 - acc: 0.9268     \n",
      "Epoch 35/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2382 - acc: 0.9250     \n",
      "Epoch 36/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2325 - acc: 0.9271     \n",
      "Epoch 37/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2340 - acc: 0.9265     \n",
      "Epoch 38/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2290 - acc: 0.9282     \n",
      "Epoch 39/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2262 - acc: 0.9294     \n",
      "Epoch 40/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2240 - acc: 0.9312     \n",
      "Epoch 41/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2190 - acc: 0.9318     \n",
      "Epoch 42/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2238 - acc: 0.9303     \n",
      "Epoch 43/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2178 - acc: 0.9311     \n",
      "Epoch 44/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2135 - acc: 0.9332     \n",
      "Epoch 45/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2164 - acc: 0.9327     - ETA: 1s - loss\n",
      "Epoch 46/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2139 - acc: 0.9320     \n",
      "Epoch 47/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2101 - acc: 0.9335     \n",
      "Epoch 48/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2127 - acc: 0.9333     \n",
      "Epoch 49/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2071 - acc: 0.9346     \n",
      "Epoch 50/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2083 - acc: 0.9351     \n",
      "Epoch 51/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2047 - acc: 0.9345     \n",
      "Epoch 52/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2033 - acc: 0.9374     \n",
      "Epoch 53/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2013 - acc: 0.9358     \n",
      "Epoch 54/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1993 - acc: 0.9365     \n",
      "Epoch 55/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1994 - acc: 0.9376     \n",
      "Epoch 56/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.2002 - acc: 0.9360     \n",
      "Epoch 57/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1994 - acc: 0.9374     \n",
      "Epoch 58/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1950 - acc: 0.9396     \n",
      "Epoch 59/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1974 - acc: 0.9378     \n",
      "Epoch 60/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1924 - acc: 0.9403     \n",
      "Epoch 61/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1922 - acc: 0.9393     \n",
      "Epoch 62/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1934 - acc: 0.9385     \n",
      "Epoch 63/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1914 - acc: 0.9402     \n",
      "Epoch 64/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1903 - acc: 0.9386     \n",
      "Epoch 65/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1883 - acc: 0.9405     \n",
      "Epoch 66/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1900 - acc: 0.9395     \n",
      "Epoch 67/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1845 - acc: 0.9422     \n",
      "Epoch 68/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1846 - acc: 0.9415     \n",
      "Epoch 69/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1828 - acc: 0.9429     \n",
      "Epoch 70/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1813 - acc: 0.9424     \n",
      "Epoch 71/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1818 - acc: 0.9410     \n",
      "Epoch 72/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1822 - acc: 0.9416     \n",
      "Epoch 73/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1804 - acc: 0.9433     \n",
      "Epoch 74/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1783 - acc: 0.9449     \n",
      "Epoch 75/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1784 - acc: 0.9424     \n",
      "Epoch 76/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1792 - acc: 0.9427     \n",
      "Epoch 77/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1735 - acc: 0.9444     \n",
      "Epoch 78/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1774 - acc: 0.9442     \n",
      "Epoch 79/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1743 - acc: 0.9448     \n",
      "Epoch 80/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1741 - acc: 0.9442     \n",
      "Epoch 81/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1715 - acc: 0.9457     \n",
      "Epoch 82/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1713 - acc: 0.9450     \n",
      "Epoch 83/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1686 - acc: 0.9460     \n",
      "Epoch 84/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1716 - acc: 0.9449     \n",
      "Epoch 85/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 1s - loss: 0.1660 - acc: 0.9459     \n",
      "Epoch 86/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1697 - acc: 0.9463     \n",
      "Epoch 87/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1679 - acc: 0.9468     \n",
      "Epoch 88/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1650 - acc: 0.9472     \n",
      "Epoch 89/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1667 - acc: 0.9466     \n",
      "Epoch 90/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1660 - acc: 0.9465     \n",
      "Epoch 91/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1673 - acc: 0.9464     \n",
      "Epoch 92/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1623 - acc: 0.9496     \n",
      "Epoch 93/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1633 - acc: 0.9476     \n",
      "Epoch 94/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1611 - acc: 0.9479     \n",
      "Epoch 95/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1622 - acc: 0.9489     \n",
      "Epoch 96/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1625 - acc: 0.9488     \n",
      "Epoch 97/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1577 - acc: 0.9501     \n",
      "Epoch 98/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1616 - acc: 0.9479     \n",
      "Epoch 99/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1625 - acc: 0.9472     \n",
      "Epoch 100/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1612 - acc: 0.9480     \n",
      "Epoch 101/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1538 - acc: 0.9499     \n",
      "Epoch 102/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1574 - acc: 0.9504     \n",
      "Epoch 103/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1543 - acc: 0.9511     \n",
      "Epoch 104/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1587 - acc: 0.9493     \n",
      "Epoch 105/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1518 - acc: 0.9511     \n",
      "Epoch 106/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1546 - acc: 0.9504     \n",
      "Epoch 107/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1546 - acc: 0.9503     \n",
      "Epoch 108/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1530 - acc: 0.9516     \n",
      "Epoch 109/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1523 - acc: 0.9503     \n",
      "Epoch 110/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1514 - acc: 0.9507     \n",
      "Epoch 111/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1506 - acc: 0.9521     \n",
      "Epoch 112/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1525 - acc: 0.9514     \n",
      "Epoch 113/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1510 - acc: 0.9518     \n",
      "Epoch 114/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1520 - acc: 0.9499     \n",
      "Epoch 115/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1496 - acc: 0.9524     \n",
      "Epoch 116/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1470 - acc: 0.9525     \n",
      "Epoch 117/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1485 - acc: 0.9517     \n",
      "Epoch 118/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1485 - acc: 0.9524     \n",
      "Epoch 119/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1476 - acc: 0.9533     \n",
      "Epoch 120/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1470 - acc: 0.9524     \n",
      "Epoch 121/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1459 - acc: 0.9525     \n",
      "Epoch 122/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1451 - acc: 0.9538     \n",
      "Epoch 123/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1443 - acc: 0.9537     \n",
      "Epoch 124/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1420 - acc: 0.9544     \n",
      "Epoch 125/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1419 - acc: 0.9541     \n",
      "Epoch 126/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1454 - acc: 0.9534     \n",
      "Epoch 127/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1427 - acc: 0.9547     - ETA: 0s - loss: 0.1422 - acc: 0.954\n",
      "Epoch 128/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1424 - acc: 0.9547     \n",
      "Epoch 129/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1391 - acc: 0.9554     \n",
      "Epoch 130/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1432 - acc: 0.9547     \n",
      "Epoch 131/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1433 - acc: 0.9539     \n",
      "Epoch 132/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1406 - acc: 0.9547     \n",
      "Epoch 133/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1410 - acc: 0.9542     \n",
      "Epoch 134/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1383 - acc: 0.9549     \n",
      "Epoch 135/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1412 - acc: 0.9540     \n",
      "Epoch 136/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1382 - acc: 0.9556     - ETA: 0s - loss: 0.1294 - acc: 0.957 - ETA: 0s - loss: 0.131\n",
      "Epoch 137/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1393 - acc: 0.9548     \n",
      "Epoch 138/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1399 - acc: 0.9553     - ETA: 0s - loss: 0.1373 - a\n",
      "Epoch 139/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1376 - acc: 0.9550     \n",
      "Epoch 140/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1379 - acc: 0.9558     \n",
      "Epoch 141/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1367 - acc: 0.9559     \n",
      "Epoch 142/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1355 - acc: 0.9567     - ETA: 0s - loss: 0.133\n",
      "Epoch 143/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1338 - acc: 0.9562     \n",
      "Epoch 144/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1378 - acc: 0.9564     \n",
      "Epoch 145/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1325 - acc: 0.9568     \n",
      "Epoch 146/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1313 - acc: 0.9574     \n",
      "Epoch 147/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1367 - acc: 0.9555     \n",
      "Epoch 148/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1336 - acc: 0.9577     \n",
      "Epoch 149/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1329 - acc: 0.9575     \n",
      "Epoch 150/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1344 - acc: 0.9555     \n",
      "Epoch 151/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1340 - acc: 0.9578     \n",
      "Epoch 152/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1344 - acc: 0.9562     \n",
      "Epoch 153/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1320 - acc: 0.9573     \n",
      "Epoch 154/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1314 - acc: 0.9572     \n",
      "Epoch 155/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1301 - acc: 0.9576     \n",
      "Epoch 156/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1326 - acc: 0.9571     \n",
      "Epoch 157/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1317 - acc: 0.9582     \n",
      "Epoch 158/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1298 - acc: 0.9584     \n",
      "Epoch 159/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1281 - acc: 0.9576     \n",
      "Epoch 160/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1307 - acc: 0.9593     \n",
      "Epoch 161/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1258 - acc: 0.9585     \n",
      "Epoch 162/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1305 - acc: 0.9570     \n",
      "Epoch 163/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1301 - acc: 0.9576     \n",
      "Epoch 164/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1296 - acc: 0.9583     \n",
      "Epoch 165/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1251 - acc: 0.9589     \n",
      "Epoch 166/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1292 - acc: 0.9579     \n",
      "Epoch 167/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 1s - loss: 0.1283 - acc: 0.9581     \n",
      "Epoch 168/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1283 - acc: 0.9584     \n",
      "Epoch 169/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1269 - acc: 0.9590     \n",
      "Epoch 170/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1276 - acc: 0.9582     \n",
      "Epoch 171/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1257 - acc: 0.9595     \n",
      "Epoch 172/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1267 - acc: 0.9589     \n",
      "Epoch 173/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1244 - acc: 0.9595     \n",
      "Epoch 174/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1237 - acc: 0.9604     \n",
      "Epoch 175/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1280 - acc: 0.9581     \n",
      "Epoch 176/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1261 - acc: 0.9584     \n",
      "Epoch 177/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1228 - acc: 0.9602     \n",
      "Epoch 178/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1260 - acc: 0.9591     \n",
      "Epoch 179/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1224 - acc: 0.9599     \n",
      "Epoch 180/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1248 - acc: 0.9590     \n",
      "Epoch 181/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1249 - acc: 0.9592     \n",
      "Epoch 182/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1220 - acc: 0.9602     \n",
      "Epoch 183/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1240 - acc: 0.9597     \n",
      "Epoch 184/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1215 - acc: 0.9599     \n",
      "Epoch 185/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1203 - acc: 0.9603     \n",
      "Epoch 186/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1224 - acc: 0.9596     \n",
      "Epoch 187/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1202 - acc: 0.9607     \n",
      "Epoch 188/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1204 - acc: 0.9615     \n",
      "Epoch 189/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1193 - acc: 0.9612     \n",
      "Epoch 190/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1177 - acc: 0.9612     \n",
      "Epoch 191/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1248 - acc: 0.9593     \n",
      "Epoch 192/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1200 - acc: 0.9601     \n",
      "Epoch 193/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1200 - acc: 0.9617     \n",
      "Epoch 194/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1191 - acc: 0.9617     \n",
      "Epoch 195/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1202 - acc: 0.9608     \n",
      "Epoch 196/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1187 - acc: 0.9614     \n",
      "Epoch 197/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1175 - acc: 0.9617     \n",
      "Epoch 198/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1190 - acc: 0.9616     \n",
      "Epoch 199/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1209 - acc: 0.9603     \n",
      "Epoch 200/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1161 - acc: 0.9613     \n",
      "Epoch 201/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1186 - acc: 0.9610     \n",
      "Epoch 202/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1189 - acc: 0.9623     \n",
      "Epoch 203/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1161 - acc: 0.9629     \n",
      "Epoch 204/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1178 - acc: 0.9617     \n",
      "Epoch 205/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1214 - acc: 0.9606     \n",
      "Epoch 206/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1165 - acc: 0.9617     \n",
      "Epoch 207/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1153 - acc: 0.9613     \n",
      "Epoch 208/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1155 - acc: 0.9620     \n",
      "Epoch 209/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1136 - acc: 0.9629     \n",
      "Epoch 210/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1166 - acc: 0.9621     \n",
      "Epoch 211/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1173 - acc: 0.9620     \n",
      "Epoch 212/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1138 - acc: 0.9614     \n",
      "Epoch 213/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1133 - acc: 0.9621     \n",
      "Epoch 214/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1113 - acc: 0.9625     \n",
      "Epoch 215/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1112 - acc: 0.9637     \n",
      "Epoch 216/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1146 - acc: 0.9630     \n",
      "Epoch 217/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1113 - acc: 0.9635     \n",
      "Epoch 218/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1132 - acc: 0.9637     \n",
      "Epoch 219/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1155 - acc: 0.9624     \n",
      "Epoch 220/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1144 - acc: 0.9627     \n",
      "Epoch 221/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1086 - acc: 0.9650     \n",
      "Epoch 222/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1120 - acc: 0.9640     \n",
      "Epoch 223/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1131 - acc: 0.9626     \n",
      "Epoch 224/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1100 - acc: 0.9643     \n",
      "Epoch 225/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1132 - acc: 0.9642     \n",
      "Epoch 226/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1096 - acc: 0.9635     \n",
      "Epoch 227/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1137 - acc: 0.9628     \n",
      "Epoch 228/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1121 - acc: 0.9639     \n",
      "Epoch 229/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1086 - acc: 0.9635     \n",
      "Epoch 230/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1092 - acc: 0.9642     \n",
      "Epoch 231/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1118 - acc: 0.9626     \n",
      "Epoch 232/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1161 - acc: 0.9629     \n",
      "Epoch 233/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1093 - acc: 0.9641     \n",
      "Epoch 234/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1146 - acc: 0.9627     \n",
      "Epoch 235/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1108 - acc: 0.9641     \n",
      "Epoch 236/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1073 - acc: 0.9643     \n",
      "Epoch 237/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1078 - acc: 0.9650     \n",
      "Epoch 238/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1111 - acc: 0.9641     \n",
      "Epoch 239/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1150 - acc: 0.9622     \n",
      "Epoch 240/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1074 - acc: 0.9651     \n",
      "Epoch 241/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1088 - acc: 0.9653     \n",
      "Epoch 242/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1084 - acc: 0.9653     \n",
      "Epoch 243/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1078 - acc: 0.9643     \n",
      "Epoch 244/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1088 - acc: 0.9658     \n",
      "Epoch 245/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1098 - acc: 0.9644     \n",
      "Epoch 246/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1057 - acc: 0.9663     \n",
      "Epoch 247/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1104 - acc: 0.9644     \n",
      "Epoch 248/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1083 - acc: 0.9640     \n",
      "Epoch 249/250\n",
      "33600/33600 [==============================] - 1s - loss: 0.1070 - acc: 0.9649     \n",
      "Epoch 250/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 1s - loss: 0.1081 - acc: 0.9648     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28f287814e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train_labels, batch_size=128, epochs=250) #, epochs=250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказываем класс объекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8160/8400 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "pred_val = model.predict_classes(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оцениваем качество решение на валидационной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.965714285714\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %s' % accuracy_score(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.99      0.98       816\n",
      "        1.0       0.99      0.99      0.99       909\n",
      "        2.0       0.96      0.97      0.97       846\n",
      "        3.0       0.97      0.96      0.96       937\n",
      "        4.0       0.98      0.96      0.97       839\n",
      "        5.0       0.96      0.97      0.97       702\n",
      "        6.0       0.93      0.98      0.96       785\n",
      "        7.0       0.96      0.97      0.96       893\n",
      "        8.0       0.97      0.93      0.95       835\n",
      "        9.0       0.96      0.95      0.95       838\n",
      "\n",
      "avg / total       0.97      0.97      0.97      8400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[811   0   1   0   1   0   3   0   0   0]\n",
      " [  0 898   0   0   2   0   5   4   0   0]\n",
      " [  3   2 818  10   2   0   3   5   2   1]\n",
      " [  0   0  10 897   0  14   1   7   6   2]\n",
      " [  1   2   1   0 806   0  12   2   2  13]\n",
      " [  0   0   0   8   1 680   8   1   3   1]\n",
      " [  5   3   0   0   3   2 772   0   0   0]\n",
      " [  1   1   9   4   2   0   0 863   1  12]\n",
      " [  8   1   8   2   3   7  22   4 774   6]\n",
      " [  9   3   2   2   4   4   0  15   6 793]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказания на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26496/28000 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "pred_test = model.predict_classes(test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализируем предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzsAAAC3CAYAAAA1tUARAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAACgkJjsAAAAACgkJjsAAAAACun/AWXGJwYhGpMYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28f2ab9cc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "for i, img in enumerate(test_img[0:5], 1):\n",
    "    subplot = fig.add_subplot(1, 7, i)\n",
    "    plt.imshow(img, cmap='gray');\n",
    "    subplot.set_title('%s' % pred_test[i - 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Готовим файл для отправки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('submit04_v07.txt', 'w') as dst:\n",
    "    dst.write('ImageId,Label\\n')\n",
    "    for i, p in enumerate(pred_test, 1):\n",
    "        dst.write('%s,%s\\n' % (i, p))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "оценка на kaggle: 0.96571"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
